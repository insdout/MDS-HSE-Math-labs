{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6. Optimization. Programming Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For grading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT CHANGE\n",
    "import grading\n",
    "import grading_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT CHANGE\n",
    "grader = grading.Grader(assignment_key=\"BiQjKqu3RIyxwEW4WigmgA\", \n",
    "                        all_parts=[\"C2ep5\", \"AVd3G\", \"Dkq1F\", \"OUwhX\", \"vjKMj\", \"uWwHK\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token expires every 30 min\n",
    "COURSERA_TOKEN = \"pgD4x5jxtB19JXKQ\"\n",
    "COURSERA_EMAIL = \"zayzev.mikhail@rambler.ru\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us consider the **House Pricing** dataset, where you have a lot of information about the houses being sold and you aim to produce the price of the house. \n",
    "\n",
    "**To submit your answers you will need to copy your token from next task ‘Programming: Final project’.**\n",
    "\n",
    "**NOTE: you should write an optimal code: try avoiding cycles and use `numpy` instead! Optimality of your code will be graded out of 2 points**\n",
    "\n",
    "Firstly, let us import basic libraries (`numpy` ([docs](https://numpy.org/)) for matrix operations and `pandas` ([docs](https://pandas.pydata.org/)) for convinient dataset workaround):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1. Reading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>condition</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-09-16</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.25</td>\n",
       "      <td>3710</td>\n",
       "      <td>34200</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>2510</td>\n",
       "      <td>1200</td>\n",
       "      <td>1986</td>\n",
       "      <td>0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.610100</td>\n",
       "      <td>-122.046997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-11-18</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.75</td>\n",
       "      <td>2820</td>\n",
       "      <td>8879</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1540</td>\n",
       "      <td>1280</td>\n",
       "      <td>1920</td>\n",
       "      <td>1957</td>\n",
       "      <td>98146</td>\n",
       "      <td>47.509399</td>\n",
       "      <td>-122.375999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-11-10</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1240</td>\n",
       "      <td>239144</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1240</td>\n",
       "      <td>0</td>\n",
       "      <td>1921</td>\n",
       "      <td>1992</td>\n",
       "      <td>98038</td>\n",
       "      <td>47.430302</td>\n",
       "      <td>-122.045998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-04-16</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2670</td>\n",
       "      <td>8279</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2670</td>\n",
       "      <td>0</td>\n",
       "      <td>1999</td>\n",
       "      <td>0</td>\n",
       "      <td>98148</td>\n",
       "      <td>47.429199</td>\n",
       "      <td>-122.328003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-07-23</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2700</td>\n",
       "      <td>4025</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>1760</td>\n",
       "      <td>940</td>\n",
       "      <td>1907</td>\n",
       "      <td>0</td>\n",
       "      <td>98122</td>\n",
       "      <td>47.607399</td>\n",
       "      <td>-122.293999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14995</th>\n",
       "      <td>2014-05-21</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.75</td>\n",
       "      <td>2290</td>\n",
       "      <td>6120</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>2170</td>\n",
       "      <td>120</td>\n",
       "      <td>1926</td>\n",
       "      <td>0</td>\n",
       "      <td>98115</td>\n",
       "      <td>47.674599</td>\n",
       "      <td>-122.327003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14996</th>\n",
       "      <td>2015-04-01</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1430</td>\n",
       "      <td>9250</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>990</td>\n",
       "      <td>440</td>\n",
       "      <td>1983</td>\n",
       "      <td>0</td>\n",
       "      <td>98052</td>\n",
       "      <td>47.695202</td>\n",
       "      <td>-122.096001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14997</th>\n",
       "      <td>2014-07-11</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>640</td>\n",
       "      <td>7768</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>640</td>\n",
       "      <td>0</td>\n",
       "      <td>1942</td>\n",
       "      <td>0</td>\n",
       "      <td>98106</td>\n",
       "      <td>47.514999</td>\n",
       "      <td>-122.359001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14998</th>\n",
       "      <td>2014-05-15</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1630</td>\n",
       "      <td>10304</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1630</td>\n",
       "      <td>0</td>\n",
       "      <td>1953</td>\n",
       "      <td>0</td>\n",
       "      <td>98155</td>\n",
       "      <td>47.754799</td>\n",
       "      <td>-122.317001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14999</th>\n",
       "      <td>2014-11-20</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>720</td>\n",
       "      <td>4592</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>720</td>\n",
       "      <td>0</td>\n",
       "      <td>1943</td>\n",
       "      <td>0</td>\n",
       "      <td>98199</td>\n",
       "      <td>47.653400</td>\n",
       "      <td>-122.403999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15000 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             date  bedrooms  bathrooms  sqft_living  sqft_lot  floors  \\\n",
       "0      2014-09-16       5.0       3.25         3710     34200     2.0   \n",
       "1      2014-11-18       3.0       1.75         2820      8879     1.0   \n",
       "2      2014-11-10       3.0       1.00         1240    239144     1.0   \n",
       "3      2015-04-16       4.0       2.50         2670      8279     2.0   \n",
       "4      2014-07-23       3.0       2.25         2700      4025     2.0   \n",
       "...           ...       ...        ...          ...       ...     ...   \n",
       "14995  2014-05-21       4.0       2.75         2290      6120     2.0   \n",
       "14996  2015-04-01       3.0       2.00         1430      9250     1.0   \n",
       "14997  2014-07-11       2.0       1.00          640      7768     1.0   \n",
       "14998  2014-05-15       3.0       1.00         1630     10304     1.0   \n",
       "14999  2014-11-20       2.0       1.00          720      4592     1.0   \n",
       "\n",
       "       waterfront  condition  grade  sqft_above  sqft_basement  yr_built  \\\n",
       "0           False          3      8        2510           1200      1986   \n",
       "1           False          5      7        1540           1280      1920   \n",
       "2           False          3      6        1240              0      1921   \n",
       "3           False          3      7        2670              0      1999   \n",
       "4           False          4      8        1760            940      1907   \n",
       "...           ...        ...    ...         ...            ...       ...   \n",
       "14995       False          4      7        2170            120      1926   \n",
       "14996       False          4      8         990            440      1983   \n",
       "14997       False          3      6         640              0      1942   \n",
       "14998       False          5      7        1630              0      1953   \n",
       "14999       False          4      6         720              0      1943   \n",
       "\n",
       "       yr_renovated  zipcode        lat        long  \n",
       "0                 0    98074  47.610100 -122.046997  \n",
       "1              1957    98146  47.509399 -122.375999  \n",
       "2              1992    98038  47.430302 -122.045998  \n",
       "3                 0    98148  47.429199 -122.328003  \n",
       "4                 0    98122  47.607399 -122.293999  \n",
       "...             ...      ...        ...         ...  \n",
       "14995             0    98115  47.674599 -122.327003  \n",
       "14996             0    98052  47.695202 -122.096001  \n",
       "14997             0    98106  47.514999 -122.359001  \n",
       "14998             0    98155  47.754799 -122.317001  \n",
       "14999             0    98199  47.653400 -122.403999  \n",
       "\n",
       "[15000 rows x 16 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datX=np.load('x_train.npy')\n",
    "datY=np.log(np.load('y_train.npy'))\n",
    "datX=pd.DataFrame(datX, columns=datX.dtype.names)\n",
    "datX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, we manage to load the data (you can read more about the `load` [here](https://docs.scipy.org/doc/numpy/reference/generated/numpy.load.html). But it is not a necessity). We are going to use linear models to work with it, but firstly we need to come up with idea what features should we include in the model at all (which feature the price is lineary dependent on):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do not forget to install seaborn. You can do that by running `pip install seaborn` in the command line locally, or simply by running the next sell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to do it let us plot every feature vs the price. Firstly, we import nice plotting modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax=plt.subplots(4, 4, figsize=(16,16))\n",
    "\n",
    "for i, name in enumerate(datX.columns):\n",
    "    ax[i//4][i%4].scatter(datX[name], datY)\n",
    "    ax[i//4][i%4].set_title(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us say, that we choose to work the following set of features:\n",
    "+ `bedrooms`\n",
    "+ `bathrooms`\n",
    "+ `sqft_living`\n",
    "+ `floors`\n",
    "+ `condition`\n",
    "+ `grade`\n",
    "+ `sqft_above`\n",
    "+ `sqft_basement`\n",
    "+ `long`\n",
    "+ `lat`\n",
    "\n",
    "Clear the dataset from all the other features and create:\n",
    "1. matrix $X$, all elements should be real numbers\n",
    "2. number $N$ -- number of considered houses\n",
    "3. number $m$ -- number of new features\n",
    "\n",
    "**Hint**: it is easier to clean columns from dataset (you should look [here](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html) for insipration) and the get a matrix with `.values`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code goes here\n",
    "feature_names = [\"bedrooms\",\"bathrooms\",\"sqft_living\",\"floors\",\n",
    "                 \"condition\",\"grade\",\"sqft_above\",\"sqft_basement\",\"long\",\"lat\"]\n",
    "X=datX[feature_names]\n",
    "N=X.shape[0]\n",
    "m=X.shape[1]\n",
    "print(N,m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cells to automatically check results of your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GRADED PART, DO NOT CHANGE!\n",
    "grader.set_answer(\"C2ep5\", grading_utils.test_reader(X, N, m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can make submission with answers so far to check yourself at this stage\n",
    "grader.submit(COURSERA_EMAIL, COURSERA_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider that we are interested in the loss of the model we discussed in the video:\n",
    "\n",
    "+ Assume we have input data that is denoted as $\\vec{x}_1, \\vec{x}_2, \\ldots, \\vec{x}_N$\n",
    "+ House prices for this input data are known $y_1, y_2, \\ldots, y_N$\n",
    "\n",
    "We propose a **simple linear model** for this task:\n",
    "\n",
    "$$ \\hat{y}_i=w_0+w_1x_1+w_2x_2+\\ldots+w_mx_m $$\n",
    "\n",
    "As a loss function we will use the mean squared error (**MSE**):\n",
    "\n",
    "$$\n",
    "Loss(\\vec{w})=\\frac{1}{N}\\sum_{i=1}^N (y_i-\\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "### Task 2. Compute analytically the $Loss(\\vec{w})$ function\n",
    "Please, keep the signature of the function and enter the code only under `your code goes here`.\n",
    "\n",
    "**Attention**: you need to avoid usage of `for` cycles! The easiest way to do it is by using matrix operations. **Your score will be decreased if you use cycles**!\n",
    "\n",
    "_Hint_: to get nice $w_0$ coefficient it is convinient to add to the `X` matrix the column of 1 with `np.concatenate` [documentation](https://docs.scipy.org/doc/numpy/reference/generated/numpy.concatenate.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(w, X, y):\n",
    "    N = X.shape[0]\n",
    "    m = X.shape[1]\n",
    "    b = np.ones([X.shape[0], 1])\n",
    "    X = np.concatenate([b, X], axis=1)\n",
    "    lossValue = (y -X@w).T @(y -X@w)/y.shape[0]\n",
    "  \n",
    "    return lossValue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cells to automatically check results of your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GRADED PART, DO NOT CHANGE!\n",
    "grader.set_answer(\"AVd3G\", grading_utils.test_loss(loss, X, datY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can make submission with answers so far to check yourself at this stage\n",
    "grader.submit(COURSERA_EMAIL, COURSERA_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3. Compute analyticaly the gradient of the $Loss(\\vec{w})$ (1 point)\n",
    "Please, enter your answer in the cell below (it should be a `markdown` cell). You can initially specify each partial derivative $\\frac{\\partial Loss}{\\partial w_i}$, but **your final answer must consists of $\\nabla Loss$ altogether using matrix operations**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$\\large \\begin{array}{rcl}\\mathcal{L}\\left(\\textbf{X}, \\textbf{y}, \\textbf{w} \\right) &=& \\frac{1}{n} \\sum_{i=1}^n \\left(y_i - \\textbf{w}^{\\text{T}} \\textbf{x}_i\\right)^2 \\\\\n",
    "&=& \\frac{1}{n} \\left\\| \\textbf{y} - \\textbf{X} \\textbf{w} \\right\\|_2^2 \\\\\n",
    "&=& \\frac{1}{n} \\left(\\textbf{y} - \\textbf{X} \\textbf{w}\\right)^{\\text{T}} \\left(\\textbf{y} - \\textbf{X} \\textbf{w}\\right)\n",
    "\\end{array}$$\n",
    "\n",
    "$$\\large \\begin{array}{rcl} \\frac{\\partial \\mathcal{L}}{\\partial \\textbf{w}} &=& \\frac{\\partial}{\\partial \\textbf{w}} \\frac{1}{n} \\left( \\textbf{y}^{\\text{T}} \\textbf{y} -2\\textbf{y}^{\\text{T}} \\textbf{X} \\textbf{w} + \\textbf{w}^{\\text{T}} \\textbf{X}^{\\text{T}} \\textbf{X} \\textbf{w}\\right) \\\\\n",
    "&=& \\frac{1}{n} \\left(-2 \\textbf{X}^{\\text{T}} \\textbf{y} + 2\\textbf{X}^{\\text{T}} \\textbf{X} \\textbf{w}\\right) = \\frac{2}{n}\\textbf{X}^{\\text{T}}\\left(\\textbf{X} \\textbf{w} -\\textbf{y} \\right)\n",
    "\\end{array} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4. Write a function to compute the gradient of the Loss function in the given point\n",
    "Please, keep the signature of the function and enter the code only under `your code goes here`. \n",
    "\n",
    "**Attention**: you need to avoid usage of `for` cycles! The easiest way to do it is by using matrix operations. **Your score will be decreased if you use cycles**!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(w_k, X, y):\n",
    "    N = X.shape[0]\n",
    "    m = X.shape[1]\n",
    "    b = np.ones([X.shape[0], 1])\n",
    "    X = np.concatenate([b, X], axis=1)\n",
    "    lossGradient = 2* X.T @(X@w_k -y)/y.shape[0]\n",
    "    \n",
    "    return lossGradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cells to automatically check your function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GRADED PART, DO NOT CHANGE!\n",
    "grader.set_answer(\"Dkq1F\", grading_utils.test_grad(grad, X, datY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can make submission with answers so far to check yourself at this stage\n",
    "grader.submit(COURSERA_EMAIL, COURSERA_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5. Write gradient descent (2 points)\n",
    "How it is time to formulate the gradient descent! As you remeember, the idea here is that:\n",
    "$$\n",
    "\\vec{w}^{k+1}=\\vec{w}^{k}-\\alpha_k\\cdot \\nabla Loss(\\vec{w}^{k}\n",
    "$$\n",
    "We propose that you use constant $\\alpha_k=\\alpha$. Assume that the method should stop in two cases:\n",
    "+ if the number of iterations is to high (`maxiter`)\n",
    "+ if the length of the gradient is low enough (<`eps`) to call an extremum\n",
    "\n",
    "Please, keep the signature of the function and enter the code only under `your code goes here`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradDescent(w_init, alpha, X, y, maxiter=500, eps=1e-2):\n",
    "    losses=[]\n",
    "    weights=[w_init]\n",
    "    curiter=0\n",
    "    w_k=weights[-1]\n",
    "    weights=weights[0].copy()\n",
    "    #your code goes here\n",
    "    while curiter < maxiter:\n",
    "        grad_k = grad(w_k, X, y)\n",
    "        if np.sqrt(grad_k@grad_k) < eps:\n",
    "            print(\"Converged at {} iteration\".format(curiter))\n",
    "            break\n",
    "        w_k -=  2 * alpha * grad_k\n",
    "        lossValue_k=loss(w_k, X, y)\n",
    "        weights=np.vstack((weights,w_k))\n",
    "        losses.append(lossValue_k)\n",
    "        curiter += 1\n",
    "        if curiter%100 == 0:\n",
    "            print(\"iter:\",curiter,\"grad len:\", np.sqrt(grad_k.dot(grad_k)),\"loss:\",lossValue_k)\n",
    "        \n",
    "    return weights, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with several alphas and several intial values of weights. To illustrate, provide graphs for the Loss function over iterations in each case (and, optionally, the distance between weigths from one iteration to the next):\n",
    "\n",
    "(we provided all key plotting commands for you, but you can always look into [this tutorial](https://matplotlib.org/tutorials/introductory/usage.html#sphx-glr-tutorials-introductory-usage-py))\n",
    "\n",
    "**Note:** You need to provide at least **two** experiments with **different values of $\\alpha$** (**1 point**). Preferably, there should be at least one convergent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X=datX[[\"bedrooms\",\"bathrooms\",\"sqft_living\",\"floors\",\"condition\",\n",
    "        \"grade\",\"sqft_above\",\"sqft_basement\",\"long\",\"lat\"]].copy()\n",
    "y = datY.copy()\n",
    "\n",
    "print(\"First experiment:\")\n",
    "w_init = np.random.normal(size=(X.shape[1]+1), scale=0.1)\n",
    "alpha = 0.1e-7\n",
    "weights_1, losses_1 = gradDescent(w_init, alpha, X, y, maxiter=500, eps=1e-2)\n",
    "dist1 = [np.linalg.norm(weights_1[i+1]-weights_1[i]) for i in range(len(weights_1)-1)]\n",
    "\n",
    "print(\"Second experiment:\")\n",
    "alpha = 0.55e-7\n",
    "w_init = np.random.normal(size=(X.shape[1]+1), scale=0.1)\n",
    "weights_2, losses_2 = gradDescent(w_init, alpha, X, y, maxiter=500, eps=1e-2)\n",
    "dist2 = [np.linalg.norm(weights_2[i+1]-weights_1[i]) for i in range(len(weights_2)-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(losses_1, dist1, losses_2, dist2, title):\n",
    "    fig, ax1 = plt.subplots(figsize=(10,8))\n",
    "    ax1.plot(losses_1, color = 'r',linewidth=3,label=\"Experiment 1 Loss\")\n",
    "    ax1.plot(losses_2,color = 'g',linewidth=3,label=\"Experiment 2 Loss\")\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(dist1, color = 'r',linestyle='dashed',label=\"Experiment 1 weights distance\")\n",
    "    ax2.plot(dist2, color = 'g',linestyle='dashed',label=\"Experiment 2 weights distance\")\n",
    "\n",
    "    ax1.set_title(title, fontsize=18)\n",
    "    ax1.set_xlabel(\"Iterations\", fontsize=15)\n",
    "    ax1.set_ylabel(\"Loss\", fontsize=15)\n",
    "    ax2.set_ylabel(\"Weights distance\", fontsize=15)\n",
    "    ax1.set_yscale('log')\n",
    "    ax2.set_yscale('log')\n",
    "    ax1.legend(loc='upper left', fontsize=15)\n",
    "    ax2.legend(loc='upper right', fontsize=15)\n",
    "    ax2.grid(False)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(losses_1, dist1, losses_2, dist2,\"Experiment 1 alpha=0.1e-7; Experiment 2 alpha=0.55e-7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check the adequacy of the model we created.\n",
    "\n",
    "Choose several (no less then five) houses (inputs in your `X` matrix) and calculte predicted prices by:\n",
    "\n",
    "$$ \\hat{y}_i=w_0+w_1x_1+w_2x_2+\\ldots+w_mx_m $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_regression(weights, losses, X, y, n=10, print_errors=True):\n",
    "    \n",
    "    ones = np.ones([X.shape[0], 1])\n",
    "    y_hat = np.concatenate([ones, X], axis=1) @ weights[-1]\n",
    "    \n",
    "    if print_errors == True:\n",
    "        #Print Y_hat:\n",
    "        print(\"First {} results:\".format(n))\n",
    "        print(\"============================================\")\n",
    "        for i in range(n):\n",
    "            print(\"True Y:\",round(y[i],2),\"Predicted Y:\",round(y_hat[i],2), \n",
    "                  \"Relative Error:\", round(100*abs((y[i]-y_hat[i])/y[i]),1),\"%\") \n",
    "        print()\n",
    "\n",
    "        #Print relative Error\n",
    "        relative_error = 100*abs((y-y_hat)/y)\n",
    "        print(\"Relative Error Analysis:\")\n",
    "        print(\"============================================\")\n",
    "        print(\"Min Relative Error:\", np.min(relative_error))\n",
    "        print(\"Mean Relative Error:\", np.mean(relative_error))\n",
    "        print(\"Max Relative Error:\", np.max(relative_error))\n",
    "        print()\n",
    "    \n",
    "    #Plot prediction and residuals:\n",
    "    print(\"Printing charts of regression predictions:\")\n",
    "    print(\"============================================\")\n",
    "    residuals = y-y_hat\n",
    "\n",
    "    fig, axs = plt.subplots(1, 3, sharex=False, sharey=False, figsize=(15,5))\n",
    "\n",
    "    axs[0].set_title(\"observed-predicted scatterplot\", fontsize=18)\n",
    "    axs[0].scatter(y_hat, y, marker=\".\")\n",
    "    axs[0].plot(range(int(min(y_hat)),int(max(y_hat))+2), range(int(min(y_hat)),int(max(y_hat))+2), \"r--\")\n",
    "    axs[0].set_xlabel(\"$\\widehat{y}$\", fontsize=15)\n",
    "    axs[0].set_ylabel(\"y\", fontsize=15)\n",
    "\n",
    "    axs[1].set_title(\"residuals-predicted scatterplot\", fontsize=18)\n",
    "    axs[1].scatter(y_hat, residuals, marker=\".\")\n",
    "    axs[1].set_xlabel(\"$\\widehat{y}$\", fontsize=15)\n",
    "    axs[1].set_ylabel(\"$y-\\widehat{y}$\", fontsize=15)\n",
    "\n",
    "    axs[2].set_title(\"residuals distribution\", fontsize=18)\n",
    "    axs[2].hist(residuals, bins=30, density=True)\n",
    "    axs[2].axvline(0, color='r', linestyle='dashed')\n",
    "    axs[2].set_xlabel(\"$y-\\widehat{y}$\", fontsize=15)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_regression(weights_1, losses_1, X, y, n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare predicted values with an actual answer (stored in your `y` array). Is it satisfying enough? (**1 point**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite of having reasonable Mean Relative Error, we can see from the 3 plots above, that the model is not adequate.  \n",
    "1. The first problem could be observed on the left plot: we can see that on $Y-\\widehat{Y}$ plot the predictions and true values don't lie on the line. That means that for some predictions we do significantly underestimate and for others we overestimate.  \n",
    "\n",
    "\n",
    "2. Same problem is observed on the middle plot: the model error is dependent on predicted value. We can see that for bigger predicted values we have bigger negative error and for small error we have large positive error. This plot ideally should be Normal distributed.\n",
    "\n",
    "3. And the third problem is observed on the right plot: its clearly seen that the error is not centered at $Y-\\widehat{Y} = 0$. So the error is biased. And the majority of error values lies below zero, so the model tends to overestimate.  \n",
    "\n",
    "The conclusion is that the model is not good enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6. Data transformation\n",
    "\n",
    "As you have probably already seen above, the convergence of the gradient descent is not ideal for our data. One way to overcome this is to transform the input data so that:\n",
    "+ the **average** of each feature should be $0$\n",
    "+ the **standard deviation** of each feature should be $1$\n",
    "\n",
    "In such a way levels of the loss function would be close to circles; thus one should hope to faster convergence.\n",
    "\n",
    "Implement such normalisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(X):\n",
    "    X_normed = (X-np.mean(X,axis=0))/ np.std(X, axis=0)\n",
    "    \n",
    "    return X_normed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_n = norm(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to automatically check your function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GRADED PART, DO NOT CHANGE!\n",
    "grader.set_answer(\"OUwhX\", grading_utils.test_norm(norm(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can make submission with answers so far to check yourself at this stage\n",
    "grader.submit(COURSERA_EMAIL, COURSERA_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7. And again (2 points)\n",
    "\n",
    "Repeat gradient descent experiments for different $\\alpha$s, now with transformed data.\n",
    "\n",
    "+ Run at least two experiments with different $\\alpha$\n",
    "+ At least two experiments should be convergent\n",
    "\n",
    "1. Provide `loss` plots for those experiments (on the same graph). (**1 point**)\n",
    "2. Are optimized weights the same? Illustrate it (plot it or show differences in other way)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_weights(weights1, weights2, feature_names):\n",
    "    feature_names_p = [\"bias\"]\n",
    "    feature_names_p.extend(feature_names)\n",
    "    labels = [\"w_{index}_{name}\".format(index=i, name=feature_names_p[i]) for i in range(len(weights1[-1]))]\n",
    "    weights1_p = weights1[-1]\n",
    "    weights2_p = weights2[-1]\n",
    "\n",
    "    x = np.arange(len(labels)) \n",
    "    width = 0.35  \n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12,5))\n",
    "    ax2 = ax.twinx()\n",
    "\n",
    "    rects1 = ax.bar(x[1:] - width/2, weights1_p[1:], width, label=\"Experiment 1\")\n",
    "    rects2 = ax.bar(x[1:] + width/2, weights2_p[1:], width, label=\"Experiment 2\")\n",
    "    bias1 = ax2.bar(x[0] - width/2, weights1_p[0], width, label=\"Experiment 1\")\n",
    "    bias2 = ax2.bar(x[0] + width/2, weights2_p[0], width, label=\"Experiment 2\")\n",
    "    ax.set_ylabel('Values')\n",
    "    ax.set_ylabel('Values bias')\n",
    "    ax.set_title('Weights')\n",
    "    \n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels, rotation=40)\n",
    "  \n",
    "    ax2.grid(False)\n",
    "    ax2.legend()\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First experiment:\")\n",
    "w_init = np.random.normal(size=(X.shape[1]+1), scale=0.5)\n",
    "X_n = norm(X)\n",
    "alpha = 0.01\n",
    "weights_1_n, losses_1_n = gradDescent(w_init, alpha, X_n, y, maxiter=500, eps=1e-2)\n",
    "dist1_n = [np.linalg.norm(weights_1_n[i+1]-weights_1_n[i]) for i in range(len(weights_1_n)-1)]\n",
    "\n",
    "print(\"Second experiment:\")\n",
    "alpha = 0.05\n",
    "w_init = np.random.normal(size=(X.shape[1]+1), scale=0.5)\n",
    "weights_2_n, losses_2_n = gradDescent(w_init, alpha, X_n, y, maxiter=500, eps=1e-2)\n",
    "dist2_n = [np.linalg.norm(weights_2_n[i+1]-weights_2_n[i]) for i in range(len(weights_2_n)-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(losses_1_n, dist1_n, losses_2_n, dist2_n,\"Experiment 1 alpha=0.01; Experiment 2 alpha=0.05\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_regression(weights_1_n, losses_1_n, X_n, y, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_weights(weights_1_n, weights_2_n, feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weight for both experiments are not identical, but they looks similar. And taking into the account that the data X was normed and absolute values of weights the differance is not too big.  \n",
    "It is obvious that the bigges impact here comes from the bias term, than goes sqft_living, lat, grade and sqft_above. 4 out of 5 of them are pretty close for both experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 8. Better pay twice (2 points)\n",
    "\n",
    "Sometimes it is essential to alter the loss function and make it assymetric. Normally, it is motivated by the task itself. For instance, in our case assume that one uses our prediction to bid for an apartment: hence if our $\\hat{y}>y$ then we will overpay, but if $\\hat{y}<y$ we will not get an apartment, but also won't lose any money. \n",
    "\n",
    "Let us introduce our new function:\n",
    "$$\n",
    "Loss(\\vec{w})=\\frac{1}{N}\\sum_{i=1}^N \\begin{cases} a(y_i-\\hat{y}_i)^2, \\quad y_i>\\hat{y}_i \\\\ b(y_i-\\hat{y}_i)^2, \\quad y_i\\le\\hat{y}_i \\end{cases}\n",
    "$$\n",
    "\n",
    "Implement new loss and new gradient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_loss(w, X, y, a, b):\n",
    "    N = X.shape[0]\n",
    "    m = X.shape[1]\n",
    "    ones = np.ones([X.shape[0], 1])\n",
    "    X = np.concatenate([ones, X], axis=1)\n",
    "    y_hat = X@w\n",
    "\n",
    "    A = (a*(y > y_hat)+b*(y<=y_hat))\n",
    "    lossValue = A*(y -y_hat).T @(y -y_hat)/y.shape[0]\n",
    "  \n",
    "    return lossValue\n",
    "\n",
    "\n",
    "def new_grad(w_k, X, y, a, b):\n",
    "    N = X.shape[0]\n",
    "    m = X.shape[1]\n",
    "    ones = np.ones([X.shape[0], 1])\n",
    "    X = np.concatenate([ones, X], axis=1)\n",
    "    y_hat = X@w_k\n",
    "    A = (a*(y > y_hat)+b*(y<=y_hat))\n",
    "    lossGradient = 2 *A* X.T@(X@w_k - y)/y.shape[0]\n",
    "    \n",
    "    return lossGradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=norm(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cells to automatically check results of your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GRADED PART, DO NOT CHANGE!\n",
    "grader.set_answer(\"vjKMj\", grading_utils.test_new_loss(new_loss, X, datY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can make submission with answers so far to check yourself at this stage\n",
    "grader.submit(COURSERA_EMAIL, COURSERA_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cells to automatically check results of your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GRADED PART, DO NOT CHANGE!\n",
    "grader.set_answer(\"uWwHK\", grading_utils.test_new_grad(new_grad, X, datY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can make submission with answers so far to check yourself at this stage\n",
    "grader.submit(COURSERA_EMAIL, COURSERA_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us experiment with new functions (**2 points**)\n",
    "\n",
    "1. Assume your data was normalised (otherwise repeat **Task 6**)\n",
    "2. Select at least two pairs of $(a,b)$ parameters such that $a_1/b_1>1$ and $a_2/b_2<1$\n",
    "3. Run **gradient descent** with new function and given parameters\n",
    "4. Make a `loss` plot for each expriment (please, provide legend!)\n",
    "5. Check whether you've got coinciding weights and _illustrate_ it\n",
    "\n",
    "You may also alter $\\alpha$ and provide more experiments on composite relation between $a/b$ and $\\alpha$ values (_optional_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_gradDescent(w_init, alpha, X, y, a, b, maxiter=500, eps=1e-2):\n",
    "    losses=[]\n",
    "    weights=[w_init]\n",
    "    curiter=0\n",
    "    w_k=weights[-1]\n",
    "    weights=weights[0].copy()\n",
    "\n",
    "    while curiter < maxiter:\n",
    "        grad_k = new_grad(w_k, X, y, a, b)\n",
    "        if np.sqrt(grad_k@grad_k) < eps:\n",
    "            print(\"Converged at {} iteration\".format(curiter))\n",
    "            break\n",
    "        w_k -=  2 * alpha * grad_k\n",
    "        lossValue_k=new_loss(w_k, X, y, a, b)\n",
    "        weights=np.vstack((weights,w_k))\n",
    "        losses.append(lossValue_k)\n",
    "        curiter += 1\n",
    "        if curiter%100 == 0:\n",
    "            print(\"iter:\",curiter,\"grad len:\", np.sqrt(grad_k.dot(grad_k)),\"loss:\",lossValue_k)\n",
    "        \n",
    "    return weights, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First experiment:\")\n",
    "w_init = np.random.normal(size=(X.shape[1]+1), scale=0.1)\n",
    "X_n = norm(X)\n",
    "\n",
    "alpha = 0.01\n",
    "a = 10\n",
    "b=1\n",
    "weights_1_new, losses_1_new = new_gradDescent(w_init, alpha, X_n, y, a, b, maxiter=500, eps=1e-2)\n",
    "dist1_new = [np.linalg.norm(weights_1_new[i+1]-weights_1_new[i]) for i in range(len(weights_1_new)-1)]\n",
    "\n",
    "print(\"Second experiment:\")\n",
    "alpha = 0.01\n",
    "a = 1\n",
    "b=10\n",
    "w_init = np.random.normal(size=(X.shape[1]+1), scale=0.1)\n",
    "weights_2_new, losses_2_new = new_gradDescent(w_init, alpha, X_n, y, a, b, maxiter=500, eps=1e-2)\n",
    "dist2_new = [np.linalg.norm(weights_2_new[i+1]-weights_2_new[i]) for i in range(len(weights_2_new)-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(losses_1_new, dist1_new, losses_2_new, dist2_new,\"Experiment 1 a=10, b=1; Experiment 2 a=1, b=10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_regression(weights_1_new, losses_1_new, X_n, y, 10, False)\n",
    "analyze_regression(weights_2_new, losses_2_new, X_n, y, 10, False)\n",
    "plot_weights(weights_1_new, weights_2_new, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cor = X_n.copy()\n",
    "X_cor[\"Target\"] = y\n",
    "correlations = X_cor.corr()\n",
    "plt.figure(figsize=(7,6))\n",
    "sns.heatmap(correlations,cmap=\"coolwarm\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_diff = abs((weights_1_new[-1] - weights_2_new[-1]))\n",
    "print(\"Biggest weights difference:\")\n",
    "print(\"================================\")\n",
    "print(*[str(feature_names[i])+\" diff:\"+str(round(weights_diff[i+1],3)) \n",
    "        for i in np.argsort(weights_diff[1:])[::-1][:5]], sep=\"\\n\")\n",
    "corr_sorted = np.argsort(np.array(correlations[\"Target\"][:-1]))[::-1]\n",
    "print()\n",
    "print(\"Biggest feature correlation:\")\n",
    "print(\"================================\")\n",
    "print(*[str(feature_names[i])+\" corr:\"+str(round(correlations[\"Target\"][i],3)) \n",
    "        for i in corr_sorted][:5], sep=\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 10. Discussion (1 point)\n",
    "Answer following questions:\n",
    "1. Have you managed to get sufficiently different weights with different $\\alpha$ or $(a,b)$ parameters of assymetry? What does it mean?\n",
    "2. Assume $a$ and $b$ are not given by the task and you need to choose them with the data. Propose a strategy of doing that (assume $a=1$ and choosing only $b$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Have you managed to get sufficiently different weights with different  𝛼  or  (𝑎,𝑏)  parameters of assymetry? What does it mean?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_weights(weights_1_new, weights_2_new, feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some differences in weights with extreme opposite values of a and b. For the first experiment **a=10**, **b=1** and for the second **a=1** and **b=10**.  The corresponding biggest absolute differences in weights are:  \n",
    "Biggest weights difference:  \n",
    "\n",
    "**sqft_living diff: 0.061**   \n",
    "**long diff: 0.043**  \n",
    "**lat diff: 0.042**  \n",
    "**bathrooms diff: 0.04**  \n",
    "**sqft_above diff: 0.031** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations[\"Target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the features except **long** are in top 5 features in terms of correlation with target value. So it all seems logical, that the most correlated features have biggest change when we modify loss function and penalize more for overestimating.  \n",
    "\n",
    "Feature **long** has very low correlation with target so big change in feature weight will not affect the prediction as much as changing weights of high correlated features. So this big change is also logical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Assume  𝑎  and  𝑏  are not given by the task and you need to choose them with the data. Propose a strategy of doing that (assume  𝑎=1  and choosing only  𝑏 )."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the question itself, I can assume that if we for some reason need the assymetric cost function, than our real costs are different for overestimating and underestimating.  \n",
    "Thus we should know what the upper bound of probability we want for overestimating. Let us call it Treshold_value.\n",
    "And our goal is to set **b** parameter such that our predictions will be overestimating the target value with $P((y_{t}-\\hat{y}_{t})>0) < Threshold\\_value\\quad$.  \n",
    "\n",
    "**Analytical approach:**\n",
    "\n",
    "Loss function: \n",
    "$$ \\mathcal{L}(y_{t} -\\hat{y}_{t})= \\begin{cases} a(y_{t}-\\hat{y}_{t})^2, \\quad y_{t}>\\hat{y}_{t} \\\\ b(y_{t}-\\hat{y}_{t})^2, \\quad y_{t}\\le\\hat{y}_{t} \\end{cases}\n",
    "$$\n",
    "Expectation of Loss function: \n",
    "$$ E_t(\\mathcal{L}(y_{t} -\\hat{y}_{t})) = a \\int_{\\hat{y}_{t}}^\\infty (y_{t}-\\hat{y}_{t})^2 f(y_{t}|\\Omega_t)\\,d{y_{t}} + b \\int_{-\\infty}^{\\hat{y}_{t}} (y_{t}-\\hat{y}_{t})^2 f(y_{t}|\\Omega_t)\\,d{y_{t}}$$\n",
    "Differentiating with respect to predictor: \n",
    "$$ 0 = a \\int_{\\hat{y}_{t}}^\\infty (y_{t}-\\hat{y}_{t}) f(y_{t}|\\Omega_t)\\,d{y_{t}} + b \\int_{-\\infty}^{\\hat{y}_{t}} (y_{t}-\\hat{y}_{t}) f(y_{t}|\\Omega_t)\\,d{y_{t}}$$ \n",
    "To  solve this analytically we need to know: $f(\\hat{y}_{t}|\\Omega_t)$ which depends on $\\Omega_t$ and $\\Omega_t$ is changing during gradient descend.  \n",
    "Thus I assume the empirical approach will be more suitable.\n",
    " \n",
    " \n",
    "**Empirical approach:**\n",
    "    \n",
    "1. Start with **b=a**, initial $\\Omega_t$ and set **Treshold_value** for the desired probability of overestimating.\n",
    "2. Obtain $\\Omega_t$ by gradient descend.\n",
    "3. Compute predictions: $\\hat{y}_{t}$\n",
    "3. Compute the CDF of $F_{y_{t} -\\hat{y}_{t}}(0)$  \n",
    "4. And while CDF  $\\quad F_{y_{t} -\\hat{y}_{t}}(0) >  Threshold\\_value\\quad $ increase **b** and start from step 2.\n",
    "\n",
    "  \n",
    "In this way we gradually will obtain the desired distribution of error residuals with target probability for overestimating. Similar to the **residuals distribution** plots on the charts below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_regression(weights_1_new, losses_1_new, X_n, y, 10, False)\n",
    "analyze_regression(weights_2_new, losses_2_new, X_n, y, 10, False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
